---
title: "Gaussian Mixture Model, K-means, and EM Algorithm"
author: "Bonnie Yuan"
date:
output: 
  pdf_document:
    fig_width: 5
    fig_height: 4
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

When working with large datasets, such as those used in Genome-Wide Association Studies (GWAS), researchers often analyze associations between genetic variants and diseases by clustering individuals based on genetic similarity.

Two widely used algorithms for identifying clusters are K-means and the Expectation-Maximization (EM) algorithm. Each method serves a distinct purpose and has its own strengths. 

K-means is a simple and efficient algorithm that partitions data into K clusters by minimizing within-cluster distance. In contrast, EM algorithm is a more flexible and robust method that handles latent (unobserved) variables that represent hidden structures or clusters in the data. It iterates between estimating these hidden cluster memberships (E-step) and updating the model parameters (M-step) to maximize the likelihood of the observed data.

This document summarizes these two algorithms and provides accompanying R code examples.

# Notation 
Here is the notation used for the two algorithms:

-   $X=\{x_1, x_2, \dots, x_n\}$ : Observed data points
-   $K$: Number of clusters
-   $\mu_k$: Mean of the k-th Gaussian component
-   $\Sigma_k$: Covariance matrix of the k-th Gaussian component
-   $\pi_k$: Mixing coefficient for the k-th component, where $\sum_{k=1}^{K} \pi_k = 1$
-   $\gamma_{ik}$: Latent (unobserved) variable indicating the probability that data point $x_i$ belongs to cluster k


# Gaussian Mixture Model (GMM)

Gaussian Mixture Models (GMM) assume that data points are generated from a mixture of several Gaussian distributions, each characterized by its own mean, covariance, and mixing coefficient. The Gaussian mixture model can be written as:
\begin{align}
  p(x)=\sum_{k=1}^K\pi_k \phi(x; \mu_k,\Sigma_k)
\end{align}

where Gaussian density $\phi(x; \mu_k,\Sigma_k)$ is the kth component of the mixture with mean $\mu_k$, and covariance $\Sigma_k$, and $\pi_k$ is mixing coefficient, where $\sum_{k=1}^{K} \pi_k = 1$ and $0\le\pi_k\le1$

Below is an example scatter plot showing Gaussian mixture model with two clusters (Fig.1).
```{r ,echo=FALSE,message=FALSE,fig.cap="Gaussian mixture model",fig.align='center'}
library(ggplot2)
library(mclust)
library(MASS)
library(tidyverse)
library(mixtools)
set.seed(2024)
n <- 500
mu <- matrix(c(0,0,
               1,3),byrow=T,ncol=2)
covar <- array(rep(NA,2*2*2), c(2,2,2))  # 3D matrix
covar[,,1] <- matrix(c(1, 0.5, 
                       0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # positive cov
covar[,,2] <- matrix(c(1,-0.5,
                       -0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # negative cov

# mixing coefficents
pi_k      <- c(0.4,0.6)                           
class <- sample(1:2, n, replace=TRUE, prob=pi_k)
data<-matrix(rep(NA,n*2), ncol=2)
for(i in 1:n){
  data[i,]=mvrnorm(1,mu=mu[class[i],],Sigma=covar[,,class[i]])
}

data_clustered <- data.frame(data, cluster = factor(class))

# scatter plot colored by cluster
ggplot(data_clustered, aes(x = X1, y = X2, color = cluster)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "X1", y = "X2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")
```


# K-Means
The K-means algorithm partitions data into K clusters. To achieve this, the algorithm defines a new binary variable, $\gamma_{ik} \in \{0,1\}$, which indicates whether the data point $x_i$ is assigned to the kth cluster. The goal of the algorithm is to find a set of centroids, $\{\mu_k\}$ such that the distance between the data points and their closest centroids is minimized. This is done by minimizing the following objective function, which is the sum of squared Euclidean distances:
\begin{align}
  J=\sum_{i=1}^n \sum_{k=1}^K \gamma_{ik} ||x_i-\mu_k||^2
\end{align}

Steps of the K-means Algorithm:

-   Initialize $\mu_k$, usually by randomly selecting K data points.
-   Assign each data point to the nearest centroid by minimizing the distance: $\gamma_{ik}=argmin_{\gamma_{ik}J}$. In this step, $\mu_k$ are kept fixed, and $\gamma_{ik}=1$ if the objective function is minimized. 
-   Update $\mu_k = argmin_{\mu_{ik}}J$. In this step, the cluster assignments $\gamma_{ik}$ are kept fixed. And the $\mu_k$ is calculated as:
\begin{align}
  \hat{\mu_k}=\frac{\sum_{i=1}^n \gamma_{ik}x_i}{\sum_{i=1}^n \gamma_{ik}}
\end{align}

These steps are repeated iteratively until the algorithm converges.

Here is an example implementation of the K-means algorithm.

```{r}
k_means <- function(data,K, max_iter=100,tol = 1e-6) {
  n <- nrow(data)  # Number of data points
  d <- ncol(data)  # Dimension of the data
  # initialization mu_k and gamma_ik
  mu_k<-data[sample(1:n, K),]
  gamma_ik <- rep(0, n)
  old_muk <- mu_k
  for (iter in 1:max_iter) {
    # first update gamma_ik
    for( i in 1:n){
      # objective function
      obj <- apply(mu_k, 1, function(mu_k) sum((data[i, ] - mu_k)^2))
      gamma_ik[i] <- which.min(obj)
    }
    # update mu_k: calculate mean for each cluster 
    for( k in 1:K){
      subpoints_k=data[gamma_ik==k,]
      if (nrow(subpoints_k) > 0) {
       mu_k[k,]<-colSums(subpoints_k) / nrow(subpoints_k)
      }
    }
    # Check for convergence
    if (sum((mu_k - old_muk)^2) < tol) {
      cat("Converged after", iter, "iterations.\n")
      break
    }
    old_muk <- mu_k
  }
  return(list(mu_k = mu_k, gamma_ik = gamma_ik))
}
```



# EM Algorithm
## Maximize Likelihood Estimate (MLE)
Recall that the GMM assumes that data is generated from a mixture of several Gaussian distributions. The log-likelihood for Gaussian mixtures is given by:

\begin{align*}
  l(\mu_k,\Sigma_k)=\sum_{i=1}^n log \sum_{k=1}^K \pi_k \phi(x; \mu_k,\Sigma_k)
\end{align*}

To estimate the parameters $\theta=\{\pi_k, \mu_k, \Sigma_k\}$, we can take derivative w.r.t each parameter, for example, for $\mu_k$, it becomes:
\begin{align*}
\frac{d}{d \mu_k}l(\mu_k,\Sigma_k)=\sum_{i=1}^n\frac{\pi_k \phi(x; \mu_k,\Sigma_k)}{\sum_{k=1}^K \pi_k \phi(x; \mu_k,\Sigma_k)}(\Sigma_k^{-1}(x_i-\mu_k))
\end{align*}

Here we define the responsibility $\gamma_{ik}$ which represents the probability that data point $x_i$ belongs to the k-th cluster:

\begin{align*}
\gamma_{ik}=\frac{\pi_k \phi(x; \mu_k,\Sigma_k)}{\sum_{k=1}^K \pi_k \phi(x; \mu_k,\Sigma_k)}
\end{align*}

By setting the derivative to 0 and solve for $\mu_k$, we can obtain the MLE for $\mu_k$:
\begin{align*}
  \hat{\mu_k} &=\frac{\sum_{i=1}^n \gamma_{ik} x_i}{\sum_{i=1}^n \gamma_{ik}}\\
  &=\frac{1}{N_k}\sum_{i=1}^n \gamma_{ik} x_i
\end{align*}
where $N_k=\sum_{i=1}^n \gamma_{ik}$, the effective number of points assigned to the k-th cluster.
 This formula closely resembles the update step in K-means clustering, except that in K-means $\gamma_{ik}$ is binary, whereas in here, it is a probability between 0 and 1.

Similarly the MLE for $\Sigma_k$ is:
\begin{align*}
  \hat{\Sigma_k} &= \frac{1}{N_k}\sum_{i=1}^n \gamma_{ik} (x_i-\mu_k)(x_i-\mu_k)^T
\end{align*}

Finally, we update the mixing coefficient $\pi_k$. Using a Lagrange multiplier to enforce the constraint $\sum_k \pi_k=1$ we obtain:

\begin{align*}
L(\lambda)=log\text{ }p(x|\theta)+\lambda (\sum_k \pi_k-1)
\end{align*}

Then we take deravative of $L(\lambda)$ w.r.t $\pi_k$ which gives
\begin{align*}
\sum_{i=1}^n\frac{\phi(x_i;\mu_k,\Sigma_k)}{\sum_{k=1}^K \pi_k \phi(x_i;\mu_k,\Sigma_k)}+\lambda=0
\end{align*}

And we sum over k, this gives $\lambda=-N$, and recall $N_k=\sum_{i=1}^n \gamma_{ik}=\sum_{i=1}^n \frac{\pi_k \phi(x; \mu_k,\Sigma_k)}{\sum_{k=1}^K \pi_k \phi(x; \mu_k,\Sigma_k)}$. 

this gives
\begin{align*}
\hat \pi_k=\frac{N_k}{N}
\end{align*}

## Alternative view of EM

Another way to understand the EM algorithm is through the concept of latent variables and the complete-data log-likelihood. In a GMM, the data X , but the cluster membership of each point is unknown. We introduce a latent variable $Z_k=\mathbf I\{\gamma_{ik}=1\}$, which indicates whether the data point $x_i$ belongs kth cluster.

The marginal log-likelihood can then be written as:
\begin{align*}
\text{log }p(x;\theta)&=\text{log }\sum_z p(x,z;\theta)\\
&=\text{log }\sum_{q(z)} \frac{ p(x,z;\theta)}{q(z)}(z)\\
&=\text{log }E_{q(z)}[\frac{ p(x,z;\theta)}{q(z)}]
\end{align*}
Applying Jensen's inequality and introducing a distribution q(z), we obtain a lower bound on the log-likelihood known as the Evidence Lower Bound (ELBO):
\begin{align*}
\text{log }p(x;\theta)&\ge E_{q(z)}[\text{log } \frac{ p(x,z;\theta)}{q(z)}]\\
&=E_{q(z)}[\text{log } p(x,z;\theta)] - E_{q(z)}[\text{log }q(z)]\\
&=Q(\theta) + H(q(z))
\end{align*}

Here, H(q(z)) is the entropy of q(z),  which measures the uncertainty in q(z), and $Q(\theta)$ is the expected complete-data log-likelihood under the distribution q(z).

-   We further define $E_{q(z)}[\text{log } \frac{ p(x,z;\theta)}{q(z)}]=ELBO(\theta,q)$, Evidence Lower Bound based on the evidence of observed data. 

Further investigate $ELBO(\theta,q)$ we'll have 

\begin{align*}
ELBO(\theta,q)&=E_{q(z)}[\text{log } \frac{ p(x,z;\theta)}{q(z)}]\\
&=\sum_{q(z)} \text{log } \frac{ p(x,z;\theta)}{q(z)}\\
&=\sum_{q(z)} \text{log } \frac{ p(x,z;\theta)}{p(z|x;\theta)} \frac{ p(z|x;\theta)}{q(z)}\\
&=\sum_{q(z)} \text{log } \frac{ p(z|x;\theta)p(x;\theta)}{p(z|x;\theta)} \frac{ p(z|x;\theta)}{q(z)}\\
&=\sum_{q(z)} \text{log }  p(x;\theta)\frac{ p(z|x;\theta)}{q(z)}\\
&=E_{q(z)}[\text{log }  p(x;\theta)]-E_{q(z)}[\text{log } \frac{ p(z|x;\theta)}{q(z)}]\\
&=\text{log }  p(x;\theta) -E_{q(z)}[\text{log } \frac{q(z)}{ p(z|x;\theta)}]
\end{align*}

We can observe that the ELBO is equal to the marginal log-likelihood minus the expected difference between the prior q(z) and the posterior distribution $p(z|x;\theta)$,which is the Kullback-Leibler (KL) divergence.This KL divergence measures how close the two distrisbution. Therefore, to optimize the ELBO effectively, the goal is to set q(z) as close as possible to the posterior $p(z|x;\theta)=\frac{\pi_k \phi(x; \mu_k,\Sigma_k)}{\sum_{k=1}^K \pi_k \phi(x; \mu_k,\Sigma_k)}$.

The general steps of the EM algorithm are:

-   Initialize $\theta=\theta^{(0)}$

-   (E-step) At step t, calculate E[q(z|x,$\theta^{(t)}$)]/responsibility and compute expected value of complete log-likelihood over q(z) $Q(\theta^{(t)})=E_{q(z)}[log\text{ }p(x,z|\theta)]$

-   (M-step) Maximize $\theta^{(t+1)}=argmax_\theta Q(\theta)$


Below is an example R code for implementing EM algorithm.
```{r, include=FALSE}
EM<-function(data,K,max_iter=100,tol = 1e-6){
  n = nrow(data)
  d = ncol(data)
  # initialization
  pi_k = rep(1/K, K)
  # initialize parameters using K-means
  kmeans_result <- k_means(data, K)
  mu_k <- kmeans_result$mu_k 
  Sigmas <- array(rep(NA,2*2*d), c(2,2,d))
  
  for(k in 1:K){
    Sigmas[,,k] <- diag(d)
  }
  gamma = matrix(0,nrow=n,ncol=K)
  gamma_old = gamma

 for (i in 1:max_iter) {
    # E-step
    for(k in 1:K){
     gamma[,k] <- apply(data, 1, function(xi) {pi_k[k] * dmvnorm(xi,mu_k[k,], Sigmas[,,k])})
    }
    gamma<-gamma/rowSums(gamma)
    if (sum(abs(gamma - gamma_old)) < tol){
      break      
    }
    gamma_old <- gamma
    
    # M-step
    n_k <- colSums(gamma) 
    # Update the mixing coefficients
    pi_k <- n_k / n
    # Update the means
    for (k in 1:K) {
      mu_k[k,] <- colSums(gamma[, k] * data) / n_k[k]
      
    Sigmas[,,k] <- 0
    for (i in 1:n) {
      diff <- matrix(data[i, ] - mu_k[k, ], nrow = 1)
      Sigmas[,,k] <- Sigmas[,,k] + gamma[i, k] * t(diff) %*% diff # summing over i
    }
    Sigmas[,,k] <- Sigmas[,,k] / n_k[k]
    }

 }
  # Return the estimated parameters
  return(list(pi_k = pi_k,gamma_k=gamma, mu_k = mu_k,Sigmas=Sigmas ))
}

```


# Simulation 

# Simulation # 1: 
Now let's compare the two algorithms using the data from Figure 1. We simulate 400 data points from two Gaussian distributions with the following characteristics:

-  **Mixing coefficients:** $\pi_1=0.4$ and $\pi_1=0.6$. 
-  **Means:** $mu_1=(0,0)$, and $mu_1=(1,3)$
-  **Covariance matrices:** The covariance matrix for the two clusters are:
\begin{equation*}
    \Sigma_1=
    \begin{pmatrix} 
    1 & 0.5 \\
    0.5 & 1 
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    \Sigma_2= 
    \begin{pmatrix} 
    1 & -0.5 \\
    -0.5 & 1 
    \end{pmatrix}
\end{equation*}

```{r}
set.seed(2024)
n <- 500
mu <- matrix(c(0,0,
               1,3),byrow=T,ncol=2)
covar <- array(rep(NA,2*2*2), c(2,2,2))  # 3D matrix
covar[,,1] <- matrix(c(1, 0.5, 
                       0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # positive cov
covar[,,2] <- matrix(c(1,-0.5,
                       -0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # negative cov

# mixing coefficients
pi_k      <- c(0.4,0.6)                           
class <- sample(1:2, n, replace=TRUE, prob=pi_k)
data<-matrix(rep(NA,n*2), ncol=2)
for(i in 1:n){
  data[i,]=mvrnorm(1,mu=mu[class[i],],Sigma=covar[,,class[i]])
}

head(data)
```


Now let's compare the performance of k-means and EM

In this simulation, we can see that the results from EM are quite similar to the true means. EM slightly outperforms K-means in estimating $\mu_1$.

```{r}
result_kmeans = k_means(data,K=2)  
print(result_kmeans$mu_k)   # means

result_em = EM(data,K=2)  
print(result_em$mu_k)   
print(result_em$pi_k) # mixing coefficients
```

## Simulation # 2 

How about two clusters that are not overlap?

```{r}
set.seed(2024)
n <- 500
mu <- matrix(c(0,0,
               5,6),byrow=T,ncol=2)
covar <- array(rep(NA,2*2*2), c(2,2,2))  # 3D matrix
covar[,,1] <- matrix(c(1, 0.5, 
                       0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # positive cov
covar[,,2] <- matrix(c(1,-0.5,
                       -0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # negative cov

# mixing coefficients
pi_k      <- c(0.4,0.6)                           
class <- sample(1:2, n, replace=TRUE, prob=pi_k)
data<-matrix(rep(NA,n*2), ncol=2)
for(i in 1:n){
  data[i,]=mvrnorm(1,mu=mu[class[i],],Sigma=covar[,,class[i]])
}

data_clustered <- data.frame(data, cluster = factor(class))

# scatter plot colored by cluster
ggplot(data_clustered, aes(x = X1, y = X2, color = cluster)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "X1", y = "X2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")
```

In cases where the clusters are well-separated and spherical, both K-means and the EM algorithm are essentially the same.

```{r}
result_kmeans = k_means(data,K=2)  
print(result_kmeans$mu_k)   # means

result_em = EM(data,K=2)  
print(result_em$mu_k)   
print(result_em$pi_k) # mixing coefficients
```




# Referneces

1. http://www.di.fc.ul.pt/~jpn/r/PRML/chapter9.html#k-means-clustering-section-9.1

2. Bishop, Christopher M. ( 2006). Pattern recognition and machine learning. New York :Springer

3. https://teng-gao.github.io/blog/2022/ems/




