---
title: "Gaussian Mixture Model, K-means, and EM Algorithm"
author: "Bonnie Yuan"
date:
output: 
  pdf_document:
    fig_width: 5
    fig_height: 4
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

When working with large datasets, such as those used in Genome-Wide Association Studies (GWAS), researchers often analyze associations between genetic variants and diseases by clustering individuals based on genetic similarity.

Two widely used algorithms for identifying clusters are K-means and the Expectation-Maximization (EM) algorithm. Each method serves a distinct purpose and has its own strengths. 

K-means is a simple and efficient algorithm that partitions data into K clusters by minimizing within-cluster distance. In contrast, EM algorithm is a more flexible and robust method that handles latent (unobserved) variables that represent hidden structures or clusters in the data. It iterates between estimating these hidden cluster memberships (E-step) and updating the model parameters (M-step) to maximize the likelihood of the observed data.

This document summarizes these two algorithms and provides accompanying R code examples.

# Notation 
Here is the notation used for the two algorithms:

-   $X=\{x_1, x_2, \dots, x_n\}$ : Observed data points
-   $K$: Number of clusters
-   $\mu_k$: Mean of the k-th Gaussian component
-   $\Sigma_k$: Covariance matrix of the k-th Gaussian component
-   $\pi_k$: Mixing coefficient for the k-th component, where $\sum_{k=1}^{K} \pi_k = 1$
-   $\gamma_{ik}$: Latent (unobserved) variable indicating the probability that data point $x_i$ belongs to cluster k


# Gaussian Mixture Model (GMM)

Gaussian Mixture Models (GMM) assume that data points are generated from a mixture of several Gaussian distributions, each characterized by its own mean, covariance, and mixing coefficient. The Gaussian mixture model can be written as:
\begin{align}
  p(x)=\sum_{k=1}^K\pi_k \phi(x; \mu_k,\Sigma_k)
\end{align}

where Gaussian density $\phi(x; \mu_k,\Sigma_k)$ is the kth component of the mixture with mean $\mu_k$, and covariance $\Sigma_k$, and $\pi_k$ is mixing coefficient, where $\sum_{k=1}^{K} \pi_k = 1$ and $0\le\pi_k\le1$

Below is an example scatter plot showing Gaussian mixture model with two clusters (Fig.1).
```{r ,echo=FALSE,message=FALSE,fig.cap="Gaussian mixture model",fig.align='center'}
library(ggplot2)
library(mclust)
library(MASS)
library(tidyverse)
library(mixtools)
set.seed(2024)
n <- 500
mu <- matrix(c(0,0,
               1,3),byrow=T,ncol=2)
covar <- array(rep(NA,2*2*2), c(2,2,2))  # 3D matrix
covar[,,1] <- matrix(c(1, 0.5, 
                       0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # positive cov
covar[,,2] <- matrix(c(1,-0.5,
                       -0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # negative cov

# mixing coefficents
pi_k      <- c(0.4,0.6)                           
class <- sample(1:2, n, replace=TRUE, prob=pi_k)
data<-matrix(rep(NA,n*2), ncol=2)
for(i in 1:n){
  data[i,]=mvrnorm(1,mu=mu[class[i],],Sigma=covar[,,class[i]])
}

data_clustered <- data.frame(data, cluster = factor(class))

# scatter plot colored by cluster
ggplot(data_clustered, aes(x = X1, y = X2, color = cluster)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "X1", y = "X2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")
```


# K-Means
The K-means algorithm partitions data into K clusters. To achieve this, the algorithm defines a new binary variable, $\gamma_{ik} \in \{0,1\}$, which indicates whether the data point $x_i$ is assigned to the kth cluster. The goal of the algorithm is to find a set of centroids, $\{\mu_k\}$ such that the distance between the data points and their closest centroids is minimized. This is done by minimizing the following objective function, which is the sum of squared Euclidean distances:
\begin{align}
  J=\sum_{i=1}^n \sum_{k=1}^K \gamma_{ik} ||x_i-\mu_k||^2
\end{align}

Steps of the K-means Algorithm:

-   Initialize $\mu_k$, usually by randomly selecting K data points.
-   Assign each data point to the nearest centroid by minimizing the distance: $\gamma_{ik}=argmin_{\gamma_{ik}J}$. In this step, $\mu_k$ are kept fixed, and $\gamma_{ik}=1$ if the objective function is minimized. 
-   Update $\mu_k = argmin_{\mu_{ik}}J$. In this step, the cluster assignments $\gamma_{ik}$ are kept fixed. And the $\mu_k$ is calculated as:
\begin{align}
  \hat{\mu_k}=\frac{\sum_{i=1}^n \gamma_{ik}x_i}{\sum_{i=1}^n \gamma_{ik}}
\end{align}

These steps are repeated iteratively until the algorithm converges.

Here is an example implementation of the K-means algorithm.

```{r}
k_means <- function(data,K, max_iter=100,tol = 1e-6) {
  n <- nrow(data)  # Number of data points
  d <- ncol(data)  # Dimension of the data
  # initialization mu_k and gamma_ik
  mu_k<-data[sample(1:n, K),]
  gamma_ik <- rep(0, n)
  old_muk <- mu_k
  for (iter in 1:max_iter) {
    # first update gamma_ik
    for( i in 1:n){
      # objective function
      obj <- apply(mu_k, 1, function(mu_k) sum((data[i, ] - mu_k)^2))
      gamma_ik[i] <- which.min(obj)
    }
    # update mu_k: calculate mean for each cluster 
    for( k in 1:K){
      subpoints_k=data[gamma_ik==k,]
      if (nrow(subpoints_k) > 0) {
       mu_k[k,]<-colMeans(subpoints_k)
      }
    }
    # Check for convergence
    if (sum((mu_k - old_muk)^2) < tol) {
      cat("Converged after", iter, "iterations.\n")
      break
    }
    old_muk <- mu_k
  }
  return(list(mu_k = mu_k, gamma_ik = gamma_ik))
}
```



# EM Algorithm
## Maximize Likelihood Estimate (MLE)
Recall the Gaussian Mixture model is in the form (1), The log-likelihood for Gaussian mixtures becomes:
\begin{align*}
  l(\mu_k,\Sigma_k)=\sum_{i=1}^n log \sum_{k=1}^K \pi_k \phi(x; \mu_k,\Sigma_k)
\end{align*}

If we estimate the parameters$\theta=\{\pi_k, \mu_k, \Sigma_k\}$, we take derivative w.r.t each parameter, for example, for $\mu_k$, it becomes:
\begin{align*}
\frac{d}{d \mu_k}l(\mu_k,\Sigma_k)=\sum_{i=1}^n\frac{\pi_k \phi(x; \mu_k,\Sigma_k)}{\sum_{k=1}^K \pi_k \phi(x; \mu_k,\Sigma_k)}(\Sigma_k(x_i-\mu_k))
\end{align*}

Here we define the responsibility as 
\begin{align*}
\gamma_{ik}=\frac{\pi_k \phi(x; \mu_k,\Sigma_k)}{\sum_{k=1}^K \pi_k \phi(x; \mu_k,\Sigma_k)}
\end{align*}
Indicating the probability that data point $x_i$ belongs to kth cluster. Therefore, set the derivative to 0 and we obtain the MLE of $\mu_k$, which is 
\begin{align*}
  \hat{\mu_k} &=\frac{\sum_{i=1}^n \gamma_{ik} x_i}{\sum_{i=1}^n \gamma_{ik}}\\
  &=\frac{1}{N_k}\sum_{i=1}^n \gamma_{ik} x_i
\end{align*}
where $N_k=\sum_{i=1}^n \gamma_{ik}$
Notice this gives the same formula as k-means, except the fact that here the  $\gamma_{ik}$ is probability between 0 and 1, while in k-means is a binary variable. 

Similarly the MLE of $\Sigma_k$ is:
\begin{align*}
  \hat{\Sigma_k} &= \frac{1}{N_k}\sum_{i=1}^n \gamma_{ik} (x_i-\mu_k)(x_i-\mu_k)^T
\end{align*}

Finally, we update the mixing coefficient $\pi_k$. Recall we have the constrain that $\sum_k \pi_k=1$. The Lagrange multiplier can be used to maximizing $\pi_k$
\begin{align*}
L(\lambda)=log\text{ }p(x|\theta)+\lambda (\sum_k \pi_k-1)
\end{align*}

taking deravative of $L(\lambda)$ w.r.t $\pi_k$ which gives
\begin{align*}
\sum_{i=1}^n\frac{\phi(x_i;\mu_k,\Sigma_k)}{\sum_{k=1}^K \pi_k \phi(x_i;\mu_k,\Sigma_k)}+\lambda=0
\end{align*}

And we sum over k, this gives $\lambda=-N$, and recall $N_k=\sum_{i=1}^n \gamma_{ik}=\sum_{i=1}^n \frac{\pi_k \phi(x; \mu_k,\Sigma_k)}{\sum_{k=1}^K \pi_k \phi(x; \mu_k,\Sigma_k)}$. 

this gives
\begin{align*}
\hat \pi_k=\frac{N_k}{N}
\end{align*}

## Alternative view of EM

Another way of viewing EM algorithm is through the concept of latent variable and complete likelihood. When we model the data as GMM, we only observe the X but not observing which cluster for each data point belongs to. We then define latent variable $Z_k=\mathbf I\{\gamma_{ik}=1\}$, which gives $Z_k=1$ if data point $x_i$ belongs kth cluster.

So the marginal log-likelihood is:
\begin{align*}
\text{log }p(x;\theta)&=\text{log }\sum_z p(x,z;\theta)\\
&=\text{log }\sum_{q(z)} \frac{ p(x,z;\theta)}{q(z)}(z)\\
&=\text{log }E_{q(z)}[\frac{ p(x,z;\theta)}{q(z)}]\\
&\ge E_{q(z)}[\text{log } \frac{ p(x,z;\theta)}{q(z)}]
\end{align*}
Here are some explaination of the  
where q(z) is some distribution for z, and the inequality is based on Jensen's inequality for concave function(log). H(q(z)) is the entropy of q(z), quantifies the average level of uncertainty, defined as $H(q(z))=\sum_{q(z)}q(z)\text{log }q(z)$. And lastly 


Here is a general receipt of EM algorithm:
-   Initialize $\theta=\theta^{(0)}$
-   (E-step) At step t, calculate E[q(z|x,$\theta^{(t)}$)]/responsibility and compute $Q(\theta^{(t)})=E_{q(z)}[log\text{ }p(x,z|\theta)]$
-   (M-step) Maximize $\theta^{(t+1)}=argmax_\theta Q(\theta)$


Below is an example R code for implementing EM algorithm.
```{r, include=FALSE}
EM<-function(data,K,max_iter=100,tol = 1e-6){
  n = nrow(data)
  d = ncol(data)
  # initialization
  pi_k = rep(1/K, K)
  # initialize parameters using K-means
  kmeans_result <- k_means(data, K)
  mu_k <- kmeans_result$mu_k 
  Sigmas <- array(rep(NA,2*2*d), c(2,2,d))
  
  for(k in 1:K){
    Sigmas[,,k] <- diag(d)
  }
  gamma = matrix(0,nrow=n,ncol=K)
  gamma_old = gamma

 for (i in 1:max_iter) {
    # E-step
    for(k in 1:K){
     gamma[,k] <- apply(data, 1, function(xi) {pi_k[k] * dmvnorm(xi,mu_k[k,], Sigmas[,,k])})
    }
    gamma<-gamma/rowSums(gamma)
    if (sum(abs(gamma - gamma_old)) < tol){
      break      
    }

    gamma_old <- gamma
    
    # M-step
    n_k <- colSums(gamma) 
    # Update the mixing coefficients
    pi_k <- n_k / n
    # Update the means
    for (k in 1:K) {
      mu_k[k] <- sum(gamma[, k] * data) / n_k[k]
    
    Sigmas[,,k] <- 0
    for (i in 1:n) {
      diff <- matrix(data[i, ] - mu_k[k, ], nrow = 1)
      Sigmas[,,k] <- Sigmas[,,k] + gamma[i, k] * t(diff) %*% diff
    }
    Sigmas[,,k] <- Sigmas[,,k] / n_k[k]
    }

 }
  # Return the estimated parameters
  return(list(pi_k = pi_k, mu_k = mu_k, gamma_k=gamma))
}

```


# Simulation 

Now let's compare the two algorithms. We first similate two clusters with overlap data points that we showed in Figure 1. 

```{r,include=FALSE}
set.seed(2024)
n <- 500
mu <- matrix(c(0,0,
               1,3),byrow=T,ncol=2)
covar <- array(rep(NA,2*2*2), c(2,2,2))  # 3D matrix
covar[,,1] <- matrix(c(1, 0.5, 
                       0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # positive cov
covar[,,2] <- matrix(c(1,-0.5,
                       -0.5,1), 
                     nrow=2, 
                     byrow=TRUE) # negative cov

# mixing coefficents
pi_k      <- c(0.4,0.6)                           
class <- sample(1:2, n, replace=TRUE, prob=pi_k)
data<-matrix(rep(NA,n*2), ncol=2)
for(i in 1:n){
  data[i,]=mvrnorm(1,mu=mu[class[i],],Sigma=covar[,,class[i]])
}
```



Now let's compare the performance of k-means and EM

Here we can see the results from EM is pretty similar to the true means. 
```{r}
result_kmeans = k_means(data,K=2)  
print(result_kmeans$mu_k)   # means

result_em = EM(data,K=2)  
print(result_em$mu_k)   
```

# Conclusion
K-means assumes clusters are spherical and equally sized, and it works well when the data is relatively simple and separable. However, EM is suited for more complex tasks which identifies sub-populations in heterogeneous datasets, where overlapping or irregularly shaped clusters are common.



